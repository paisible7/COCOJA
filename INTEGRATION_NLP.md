# ü§ñ Guide d'Int√©gration du Mod√®le NLP

## üìù Vue d'ensemble

Le syst√®me est maintenant pr√™t pour int√©grer votre mod√®le NLP. Toute la logique est centralis√©e dans le fichier [`chat/nlp_model.py`](chat/nlp_model.py).

## üöÄ √âtapes d'Int√©gration

### 1. Pr√©parer votre mod√®le

Assurez-vous d'avoir :
- Les poids du mod√®le (fichiers `.pt`, `.bin`, `.pth`, etc.)
- Le tokenizer associ√©
- Les d√©pendances n√©cessaires (transformers, torch, etc.)

### 2. Installer les d√©pendances

```bash
# Pour les mod√®les Hugging Face
pip install transformers torch

# Pour d'autres frameworks
# pip install tensorflow
# pip install jax
```

### 3. Configurer l'environnement

Copiez `.env.example` vers `.env` et configurez :

```bash
cp .env.example .env
```

Modifiez les variables dans `.env` :

```env
# Chemin vers votre mod√®le
NLP_MODEL_PATH=/path/to/your/model
NLP_MODEL_NAME=gpt2  # ou votre mod√®le

# Configuration GPU/CPU
NLP_DEVICE=cuda  # ou cpu

# Param√®tres de g√©n√©ration
NLP_MAX_LENGTH=512
NLP_TEMPERATURE=0.7
```

### 4. Modifier `chat/nlp_model.py`

#### Option A : Utiliser un mod√®le Hugging Face

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

class NLPModel:
    def __init__(self):
        model_name = os.getenv('NLP_MODEL_NAME', 'gpt2')
        device = os.getenv('NLP_DEVICE', 'cpu')
        
        self.device = torch.device(device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
    
    def generate_response(self, user_input, conversation_history=None, **kwargs):
        # Pr√©parer le prompt avec l'historique
        prompt = self._prepare_prompt(user_input, conversation_history)
        
        # Tokenizer
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # G√©n√©rer
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=kwargs.get('max_length', 512),
                temperature=kwargs.get('temperature', 0.7),
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # D√©coder
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    def _prepare_prompt(self, user_input, conversation_history):
        """Formater le prompt avec l'historique"""
        if not conversation_history:
            return user_input
        
        prompt = ""
        for msg in conversation_history[-5:]:  # Garder les 5 derniers messages
            role = msg['role'].upper()
            content = msg['content']
            prompt += f"{role}: {content}\n"
        
        prompt += f"USER: {user_input}\nASSISTANT:"
        return prompt
```

#### Option B : Utiliser votre propre mod√®le

```python
class NLPModel:
    def __init__(self):
        # Charger votre mod√®le custom
        self.model = self._load_custom_model()
    
    def _load_custom_model(self):
        # Votre code de chargement
        import your_model_library
        model = your_model_library.load_model('path/to/model')
        return model
    
    def generate_response(self, user_input, conversation_history=None, **kwargs):
        # Votre logique de g√©n√©ration
        response = self.model.predict(user_input)
        return response
```

### 5. Tester l'int√©gration

```bash
# Lancer le serveur
python manage.py runserver

# Dans un autre terminal, tester l'API
curl -X POST http://127.0.0.1:8000/api/chat/ask/ \
  -H "Content-Type: application/json" \
  -d '{"question": "Bonjour, comment √ßa va ?"}'
```

## üîß Configuration avanc√©e

### Gestion de la m√©moire

Pour les mod√®les volumineux :

```python
class NLPModel:
    def __init__(self):
        # Charger en 8-bit ou 4-bit
        from transformers import BitsAndBytesConfig
        
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto"
        )
```

### Cache de r√©ponses

```python
from functools import lru_cache

class NLPModel:
    @lru_cache(maxsize=100)
    def generate_response(self, user_input, **kwargs):
        # Utiliser le cache pour les questions fr√©quentes
        return self._generate(user_input, **kwargs)
```

### Gestion asynchrone

Pour ne pas bloquer les requ√™tes :

```python
from asgiref.sync import sync_to_async

# Dans views.py
async def ask_model(request):
    response = await sync_to_async(generate_ai_response)(user_input)
    return Response({'answer': response})
```

## üìä Monitoring et Logs

Ajoutez des logs pour suivre les performances :

```python
import logging

logger = logging.getLogger(__name__)

class NLPModel:
    def generate_response(self, user_input, **kwargs):
        import time
        start = time.time()
        
        response = self._generate(user_input, **kwargs)
        
        elapsed = time.time() - start
        logger.info(f"G√©n√©ration termin√©e en {elapsed:.2f}s")
        
        return response
```

## üß™ Tests

Cr√©ez des tests dans `chat/tests.py` :

```python
from django.test import TestCase
from chat.nlp_model import get_nlp_model

class NLPModelTestCase(TestCase):
    def test_model_loading(self):
        model = get_nlp_model()
        self.assertIsNotNone(model)
    
    def test_generate_response(self):
        model = get_nlp_model()
        response = model.generate_response("Hello")
        self.assertIsInstance(response, str)
        self.assertTrue(len(response) > 0)
```

## üö® D√©pannage

### Erreur : Out of Memory

- R√©duire `NLP_MAX_LENGTH`
- Utiliser `NLP_DEVICE=cpu`
- Quantifier le mod√®le (8-bit ou 4-bit)

### Erreur : Model not found

- V√©rifier `NLP_MODEL_PATH` et `NLP_MODEL_NAME`
- T√©l√©charger le mod√®le manuellement
- V√©rifier les permissions du dossier

### R√©ponses lentes

- Utiliser un GPU : `NLP_DEVICE=cuda`
- R√©duire `NLP_MAX_LENGTH`
- Impl√©menter un cache
- Utiliser un mod√®le plus petit

## üìö Exemples de mod√®les

### Mod√®les recommand√©s

| Mod√®le | Taille | Usage | Performance |
|--------|--------|-------|-------------|
| GPT-2 | 774M | Tests/Dev | Rapide |
| DistilGPT-2 | 355M | Prod l√©g√®re | Tr√®s rapide |
| GPT-J-6B | 6B | Prod avanc√©e | Moyen |
| LLaMA-2-7B | 7B | Prod premium | Moyen |

### Configuration example pour GPT-2

```python
# Dans chat/nlp_model.py
def _initialize_model(self):
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    
    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    self.model = GPT2LMHeadModel.from_pretrained('gpt2')
    self.tokenizer.pad_token = self.tokenizer.eos_token
```

## ‚úÖ Checklist avant production

- [ ] Mod√®le test√© et fonctionnel
- [ ] Variables d'environnement configur√©es
- [ ] Logs et monitoring en place
- [ ] Tests unitaires √©crits
- [ ] Gestion des erreurs impl√©ment√©e
- [ ] Performance optimis√©e (GPU/cache)
- [ ] Documentation √† jour

## üîó Ressources

- [Hugging Face Models](https://huggingface.co/models)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Django Async Views](https://docs.djangoproject.com/en/4.2/topics/async/)

---

**Note**: Le syst√®me fonctionne actuellement en mode simulation. Suivez ce guide pour int√©grer votre mod√®le r√©el.
